---
name: dev-loop
description: Standard development SOP for ALL code changes â€” bug fixes, features, refactors, any implementation. Defines the universal pipeline (investigate â†’ fix â†’ test â†’ deploy â†’ QA) and scope-based decision matrix. Also provides continuous scanning mode for autonomous operation. Use when implementing ANY code change, fixing bugs, building features, or when user says "start dev loop", "cowork", "fix this", "implement this", "build this". Each workspace can override with local skill.
user-invocable: false
---

# Dev Loop â€” Universal Development SOP

> **Part of:** Autonomous Issue Dispatch System
> See `~/.claude/skills/autonomous-issue-dispatch/SKILL.md` for full architecture.

## ğŸš¨ This Skill Applies to ALL Development Work

This is NOT just for continuous scanning. **Every code change** â€” whether from a direct user request, a GitHub issue, a Slack bug report, or autonomous scanning â€” follows this SOP. The only variable is **scope**, which determines which steps are mandatory.

### Scope: What This SOP Covers

**IN SCOPE (dev-loop applies):**
- Project/application code that lives in a repository
- Code that will be maintained, deployed, used by others
- Bug fixes, features, refactors to existing codebase

**OUT OF SCOPE (dev-loop does NOT apply):**
- Temporary scripts Claude writes for one-off automation (sync scripts, data transforms, scratchpad utilities)
- Context engineering files (CLAUDE.md, skills, agents) â€” these have their own governance
- Documentation-only changes (.md, .txt)

**Signal words for exemption:** When writing a temporary script, mention "temporary script", "one-off", or "scratchpad" in the conversation â€” the stop hook recognizes these as exempt from dev-loop enforcement.

---

## Development Pipeline (Universal)

```
ANY code change request
  â”‚
  â–¼
â”Œâ”€â”€â”€ SCOPE ASSESSMENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  How many files? What's the blast radius?                   â”‚
â”‚                                                             â”‚
â”‚  SMALL (1-3 files, clear fix, no architecture decisions)    â”‚
â”‚  â†’ Investigate â†’ Fix â†’ Test â†’ Deploy â†’ QA                  â”‚
â”‚                                                             â”‚
â”‚  MEDIUM (4+ files, crosses modules, needs planning)         â”‚
â”‚  â†’ Investigate â†’ Plan â†’ Fix â†’ Test â†’ Deploy â†’ QA           â”‚
â”‚                                                             â”‚
â”‚  LARGE (new system, schema change, architecture)            â”‚
â”‚  â†’ CTO Agent orchestrates full pipeline                     â”‚
â”‚  â†’ Or defer if autonomous (big work rule)                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step-by-Step Pipeline

| # | Step | Small | Medium | Large | Details |
|---|------|-------|--------|-------|---------|
| 1 | **Investigate** | âœ… | âœ… | âœ… | Understand the problem. Read code, reproduce, root cause. |
| 2 | **Plan** | Skip | âœ… | âœ… | Use EnterPlanMode or CTO Agent. Get user approval. |
| 3 | **Fix** | âœ… | âœ… | âœ… | Implement the change. |
| 4 | **Test** | âœ… | âœ… | âœ… | Run test suite. Add/update tests if touching testable logic. |
| 5 | **Commit** | âœ… | âœ… | âœ… | Commit with descriptive message. |
| 6 | **Deploy** | âœ… | âœ… | âœ… | Per repo's deploy mode (auto-prod, auto-stage, approval-required). |
| 7 | **QA Submit** | âœ… | âœ… | âœ… | Invoke the project's `qa-submission` skill (`.claude/skills/qa-submission/SKILL.md`). Do NOT post to Slack directly. If no skill exists, report gap to orchestrator. **No exceptions.** |
| 8 | **Report** | âœ… | âœ… | âœ… | Tell user: what changed, deployed (yes/no), QA status. |

**Steps 4-7 are MANDATORY for every code change.** The Stop hook enforces this.

### ğŸš¨ No Debug Code to Production

**Never deploy diagnostic/debug code to production.** This includes:
- Visible UI indicators (`[debug]`, `[no source]`, red text markers)
- Temporary state dumps rendered in the UI
- "Is this working?" test elements

**Debug locally first.** If you need runtime diagnostics:
- Use `console.log()` (invisible to users, visible in DevTools)
- Use local dev server (`npm run dev`)
- Ask user to check browser console

**Why:** Deploying debug code to production treats prod as staging. Even "harmless" indicators violate user trust and professional standards.

### CTO Agent â€” What It Is and How to Invoke

**The CTO Agent is `strategic-cto-planner`** â€” a specialized orchestrator that coordinates implementation work.

**To invoke the CTO Agent:**
```
Task tool with subagent_type: "strategic-cto-planner"
```

**What CTO Agent does:**
- Delegates ALL coding to `developer` agent (never writes code itself)
- Coordinates: Developer â†’ QA Engineer â†’ Integration Tester
- Enforces TDD (tests written AND executed)
- Validates completion gate before declaring done
- Maintains objectivity by not implementing

**CTO Agent orchestration flow:**
```
You â†’ CTO Agent (strategic-cto-planner)
         â†“
    Developer agent (writes code + tests)
         â†“
    QA Engineer agent (reviews + validates)
         â†“
    Integration Tester agent (E2E validation)
         â†“
    Completion Gate (all criteria verified)
```

**ğŸš¨ NEVER dispatch directly to `developer` agent for Continuous Scan Mode.** Always go through CTO. The separation prevents confirmation bias where implementers evaluate their own work.

**When CTO Agent is required:**
- Continuous Scan Mode (bug-intake, dev-loop scanning)
- Medium/Large scope work (4+ files, architecture decisions)
- Any work that needs orchestrated validation

**When CTO Agent is optional:**
- Ad-Hoc Mode with small scope (see below)

### When User Says "Don't Bother with QA" or "Just a Quick Fix"

Follow the pipeline anyway. The user's CLAUDE.md says "EVERY change gets submitted to QA. No exceptions based on perceived simplicity." If the user explicitly overrides in the moment, comply but note it.

### QA Submission â€” Skill, Not Raw API

The qa-submission skill encapsulates the full QA workflow (GitHub issue, Slack formatting, labels, poka-yoke). Posting directly to a QA channel via Slack API bypasses all of this.

**Correct:** Invoke `.claude/skills/qa-submission/SKILL.md` (per-project)
**Wrong:** Call Slack MCP tools directly to post a QA message
**Wrong:** Skip QA because "it's a small fix"

If the current project has no qa-submission skill, report this gap to the orchestrator rather than improvising.

---

## Ad-Hoc Mode (Direct User Request)

**ğŸš¨ STRICT RULE: Ad-Hoc Mode ONLY applies when ALL conditions are met:**

1. User gives a **specific, single task** directly in conversation
2. User does **NOT** invoke any skill (bug-intake, dev-loop, dispatch, etc.)
3. User does **NOT** say "scan", "intake", "check bugs", "run the loop", "cover all", etc.

**If user invokes ANY skill or mentions scanning/batch work:**
â†’ That's **Continuous Scan Mode** â†’ Use full pipeline with CTO Agent (see below)

| User says... | Mode | Why |
|-------------|------|-----|
| "Fix this specific bug in file X" | Ad-Hoc âœ… | Single, specific task |
| "Change button color to blue" | Ad-Hoc âœ… | Single, specific task |
| "Run bug intake" | **Continuous Scan** âŒ | Invokes skill |
| "Scan Slack for bugs" | **Continuous Scan** âŒ | "Scan" = batch |
| "Fix all the bugs" / "Cover all gaps" | **Continuous Scan** âŒ | Batch = scan |

---

When Ad-Hoc Mode applies:

```
User request â†’ Investigate â†’ Fix â†’ Test â†’ Commit â†’ Deploy â†’ QA â†’ Report
```

**No intake scan. No dispatcher triage. No iteration loop.** Just the pipeline.

**Decision matrix for direct requests:**

| User says... | Scope | Action |
|-------------|-------|--------|
| "Fix this bug" / "This is broken" | Usually small | Pipeline steps 1,3-8 |
| "Build this feature" / "Implement this" | Usually medium | Pipeline steps 1-8 |
| "Refactor X" / "Redesign Y" | Usually large | CTO Agent or defer |
| "Just change this text" / "Rename X" | Small | Pipeline steps 3-8 (investigate optional) |

---

## Continuous Scan Mode

**Trigger:** "Start dev loop" / "Run continuous scan" / "Scan and fix bugs" / "Cowork with <TEAMMATE>"

Uses `~/.claude/skills/bug-intake/SKILL.md` for each scan iteration.

```
START (silently - no announcement message)
  â”‚
  â–¼
â”Œâ”€â”€â”€ ITERATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                             â”‚
â”‚  PHASE 1: INTAKE SCAN                                       â”‚
â”‚  Run bug-intake skill (full scan of configured channels)    â”‚
â”‚  Intake sources: Slack channels, Google Sheets (per-repo)   â”‚
â”‚  Also checks: GitHub issues (existing, not from intake)     â”‚
â”‚  Identifies: new bugs, verification responses, questions    â”‚
â”‚                                                             â”‚
â”‚  PHASE 2: TRIAGE (Dispatcher)                               â”‚
â”‚  Run issue-dispatcher for the repo's GitHub issue queue:    â”‚
â”‚    - Hygiene checks (labels, staleness, duplicates)         â”‚
â”‚    - Categorize: Actionable / Blocked / Backlog / Needs QA  â”‚
â”‚    - Prioritize: severity > effort > dependencies > impact   â”‚
â”‚      Age is minor tiebreaker: newer first (more relevant)   â”‚
â”‚    - Issues from Phase 1 AND pre-existing GitHub issues     â”‚
â”‚      are triaged together â€” one unified queue               â”‚
â”‚                                                             â”‚
â”‚  PHASE 3: PROCESS ACTIONABLE ISSUES                         â”‚
â”‚  For each actionable issue (in priority order):             â”‚
â”‚                                                             â”‚
â”‚  Actionable issue â†’                                         â”‚
â”‚    1. Run issue-handler:                                    â”‚
â”‚       a. Ensure context (self-investigate: logs, codebase,  â”‚
â”‚          database, Slack thread, related issues)             â”‚
â”‚       b. If genuinely ambiguous: ask specific question,     â”‚
â”‚          label needs-info, move to Blocked, continue        â”‚
â”‚       c. Diagnose: query logs, reproduce, root cause        â”‚
â”‚    2. Hand off diagnosis package to CTO Agent               â”‚
â”‚       (strategic-cto-planner)                               â”‚
â”‚    3. CTO orchestrates: Developer, QA Engineer,             â”‚
â”‚       Integration Tester, Completion Gate                   â”‚
â”‚    4. TDD â€” all tests must pass before deploy               â”‚
â”‚    5. Deploy per repo's deploy mode                         â”‚
â”‚    6. Communicate result (per bug-intake Step 7)            â”‚
â”‚    7. Submit for QA (per qa-submission, if applicable)      â”‚
â”‚    8. Log: "Fixed #N, deployed/awaiting approval"           â”‚
â”‚                                                             â”‚
â”‚  Big work (flagged by Dispatcher) â†’                         â”‚
â”‚    1. If from Slack: Add :hourglass: reaction               â”‚
â”‚    2. Create/update GitHub issue                            â”‚
â”‚    3. Reply in thread: "Noted â€” needs owner approval.       â”‚
â”‚       Created issue #N for tracking."                       â”‚
â”‚    4. Log: "Deferred #N (big work)"                         â”‚
â”‚    âš ï¸ Do NOT attempt big work autonomously.                 â”‚
â”‚                                                             â”‚
â”‚  Verification response (reporter confirms) â†’                â”‚
â”‚    1. Close GitHub issue                                    â”‚
â”‚    2. Replace :eyes: with :white_check_mark:                â”‚
â”‚    3. Log: "Closed #N (verified)"                           â”‚
â”‚                                                             â”‚
â”‚  Verification response (reporter/QA rejects) â†’              â”‚
â”‚    1. Read rejection details                                â”‚
â”‚    2. Route back to CTO Agent (skip Handler â€” diagnosis     â”‚
â”‚       exists, just needs iteration)                         â”‚
â”‚    3. CTO re-fixes, tests, deploys                          â”‚
â”‚    4. Re-submit for QA                                      â”‚
â”‚    5. Update thread with new fix details                    â”‚
â”‚    6. Log: "Re-fixed #N after rejection"                    â”‚
â”‚                                                             â”‚
â”‚  Direct question from team member â†’                         â”‚
â”‚    1. Read question context                                 â”‚
â”‚    2. Reply in thread with answer                           â”‚
â”‚    3. If question requires owner decision, say so           â”‚
â”‚    4. Log: "Answered question in thread"                    â”‚
â”‚                                                             â”‚
â”‚  Big work owner response (in :hourglass: thread) â†’          â”‚
â”‚    1. Read owner's response                                 â”‚
â”‚    2. Add :eyes: to the response (mark as processed)        â”‚
â”‚    3. If approved: remove :hourglass:, route to CTO Agent   â”‚
â”‚    4. If answered questions: update GitHub issue with        â”‚
â”‚       answers, re-evaluate scope (may become actionable)    â”‚
â”‚    5. If "not now" / "backlog": log, keep deferred          â”‚
â”‚    6. Log: "Owner responded to #N: [approved/answered/      â”‚
â”‚       deferred]"                                            â”‚
â”‚                                                             â”‚
â”‚  PHASE 3B: DISPATCH TRACKING                                â”‚
â”‚  Check status of previously dispatched issues               â”‚
â”‚  (issues set to "In Progress" on project board):            â”‚
â”‚    - Query GitHub for in-progress issues in this repo       â”‚
â”‚    - For each: check if closed, merged PR, QA passed,       â”‚
â”‚      or has new comments/activity since last check           â”‚
â”‚    - Report status changes in iteration log:                 â”‚
â”‚      "Dispatched #172 â†’ PR merged, awaiting QA"             â”‚
â”‚      "Dispatched #165 â†’ still in progress (no activity)"    â”‚
â”‚      "Dispatched #134 â†’ completed, QA passed, closed"       â”‚
â”‚    - If a dispatched issue was completed by another session, â”‚
â”‚      verify: was QA submitted? Was issue closed properly?    â”‚
â”‚      Flag any issues that were closed without QA.            â”‚
â”‚                                                             â”‚
â”‚  Why: Dispatch is NOT fire-and-forget. The loop that         â”‚
â”‚  dispatched work is responsible for tracking it to           â”‚
â”‚  completion and reporting back to the operator.              â”‚
â”‚                                                             â”‚
â”‚  PHASE 4: WAIT                                              â”‚
â”‚  Sleep 5 minutes (300 seconds)                              â”‚
â”‚  Then go back to PHASE 1                                    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Big Work Lifecycle (Async via Slack)

**No terminal presence assumption.** The owner may not be watching. All persistent communication goes through Slack.

When a deferred item needs clarification or approval:

1. **Post questions as a reply** in the original `:hourglass:` Slack thread
2. **Tag the owner** in the reply
3. **Each loop iteration** scans `:hourglass:` threads for owner responses:
   - Owner approved â†’ remove `:hourglass:`, implement via CTO agent
   - Owner answered questions â†’ update GitHub issue, re-evaluate scope
   - Owner said "not now" / "backlog" â†’ keep deferred, stop asking
   - No response â†’ wait (deadline rules apply per local override)
4. **Add `:eyes:` to each processed owner reply** in the thread (prevents double-processing)

**Why Slack, not terminal:** The owner may close the terminal session, miss questions in chat history, or not be present during autonomous loops. Slack threads are persistent and async â€” the owner responds when available, and the next loop iteration picks it up.

### Exit Conditions

| Condition | Action |
|-----------|--------|
| **3 consecutive empty scans** (15 min idle) | Log summary in chat, exit silently |
| **User returns** ("stop", "I'm back", Ctrl+C) | Log summary in chat, exit silently |
| **Critical failure** (deploy fails, API down) | Log summary + blocker details, exit |
| **Context window pressure** | Log summary, exit (user re-invokes in fresh session) |

**No Slack announcements for session start/end.** Only post to Slack for actual work items.

### Iteration Logging

After each iteration, maintain a running log in chat:

```
â”€â”€ Iteration 1 (14:30) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Fixed #134 "caption not saving" â†’ tests passed â†’ deployed
â€¢ Closed #131 (verified by reporter)
â€¢ Deferred: "redesign sidebar" (big work)
â€¢ Dispatched: #172 (search rearchitecture), #165 (train button)
â³ Waiting 5 minutes...

â”€â”€ Iteration 2 (14:35) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ No new bugs
â€¢ No verification responses
â€¢ Dispatch tracking: #172 â†’ PR opened by dev agent | #165 â†’ no activity yet
â³ Waiting 5 minutes... (idle: 1/3)

â”€â”€ Iteration 3 (14:40) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ No new bugs
â€¢ Dispatch tracking: #172 â†’ merged, QA submitted | #165 â†’ in progress (3 commits)
â³ Waiting 5 minutes... (idle: 2/3)
```

### Session Summary (On Exit)

Log summary in chat only (NO Slack post).

**ğŸš¨ MANDATORY: Include timestamps** â€” the chat interface does NOT show when messages were generated. The user may read this summary hours later and needs to know exactly when the session ran and when this summary was produced.

```
Dev loop complete. N iterations. (YYYY-MM-DD HH:MM â€” HH:MM local)

Fixed (this session): #134, #135, #137
Verified: #131, #134
Pending verification: #137 (awaiting reporter)

Dispatched & tracking:
  #172 search rearchitecture â†’ merged, QA submitted, awaiting <TEAMMATE>
  #165 train button â†’ in progress (3 commits, no PR yet)
  #134 tinder swipe â†’ no activity

Deferred (needs owner): "redesign sidebar"

_Summary generated: YYYY-MM-DD HH:MM local_
```

- First line: session time range (start â€” end)
- Last line: when this specific summary text was generated (use system clock at time of writing)

### Persistent Session Log

Session logs are ephemeral (in-chat only) by default. To enable cross-session continuity, persist a compact summary to the project's auto memory directory.

**File:** `memory/dev-loop-log.md` (in the project's auto memory directory, e.g., `~/.claude/projects/{project-hash}/memory/`)

**On session start:**
1. Read `memory/dev-loop-log.md` if it exists
2. Use the last 2-3 sessions as context: pending QA items, deferred work, dispatch tracking, stalled issues
3. This avoids re-scanning already-fixed issues or losing track of dispatch status

**On each iteration end (POKA-YOKE â€” crash-safe persistence):**
1. Overwrite the CURRENT SESSION's entry in `memory/dev-loop-log.md` with the latest cumulative state
2. This ensures the log survives session crashes, context pressure exits, or terminal closures
3. On session exit, the final iteration write IS the permanent record â€” no separate exit step needed

**Entry format (overwritten each iteration, prepended as newest-first):**
1. Update the current session entry in `memory/dev-loop-log.md`
2. Use this compact format:
```
## YYYY-MM-DD HH:MM â€” HH:MM Trigger Name (N iterations, ~duration)
**Fixed:** #N (brief desc), #M (brief desc)
**Closed:** #N, #M (reason)
**QA Submitted:** #N, #M
**Pending QA:** #N, #M, ...
**Board Cleanup:** #N, #M â†’ new status
**Deferred:** #N (reason), ...
**Answered:** #N (what), ...
**Lessons:** (brief or "none")
**Last updated:** YYYY-MM-DD HH:MM local
```
- Header: `HH:MM â€” HH:MM` = session start â€” session end (or "ongoing" if mid-session)
- Footer: `Last updated` = system clock at time of writing this entry (crash-safe: if session dies, this shows when the last iteration completed)
3. **Rotation:** If the file has more than 10 session entries, trim the oldest ones.

**Why:** Without persistence, each session starts blind â€” re-discovers the same pending QA items, re-checks stalled dispatches, and loses track of what was deferred. 30 seconds of writing on exit saves 5+ minutes of redundant scanning on next start.

---

### ğŸš¨ PHASE 5: Big Work Enrichment (Before Exit)

**When the session has deferred items (big work), do NOT just list them and exit.** Before exiting, run an enrichment pass to make deferred issues dispatchable for a future session.

**Procedure:**

1. **Read every deferred GitHub issue thoroughly** â€” body, all comments, linked Slack threads
2. **Analyze what's already answered** vs what's genuinely ambiguous
3. **Only then ask the owner** about genuine gaps that would block implementation
4. **Update GitHub issues** with the owner's answers so future sessions have full context

**Rules:**
- **Exhaust existing context first.** If the answer is in the issue body or comments, do NOT ask the owner. This wastes their time and signals you didn't read.
- **Ask specific product decisions**, not implementation details you can figure out yourself.
- **Batch questions per issue** â€” one prompt per issue, not one per question.
- **If an issue has zero gaps**, report it as "ready to dispatch" and offer to dispatch immediately.

**Why this matters:** Deferred issues rot. Without enrichment, the same questions get re-asked every session, the owner gets frustrated, and issues stay blocked indefinitely. One enrichment pass during the owner's presence can unblock weeks of autonomous work.

**Flow:**
```
Exit condition met (3 empty scans / user returns)
  â”‚
  â–¼
Any deferred big work items this session?
  â”‚
  â”œâ”€ NO â†’ Log session summary, exit
  â”‚
  â””â”€ YES â†’ For each deferred item:
            1. Read GitHub issue + all context
            2. Identify genuine gaps (not already answered)
            3. Present to owner: "These N issues are ready, these M need decisions"
            4. Ask gap questions for blocked items
            5. Update GitHub issues with answers
            6. Dispatch any newly-unblocked items if owner approves
            7. THEN log session summary and exit
```

### ğŸš¨ "Dispatch" = Spawn Implementation, Not Bookkeeping

**Dispatch means starting actual work.** Adding a comment to a GitHub issue and moving it to "In Progress" on the board is NOT dispatch â€” it's bookkeeping.

**Real dispatch:**
1. Spawn CTO Agent (`strategic-cto-planner`) with the issue context
2. CTO orchestrates developer agent to write code
3. Code is committed, tested, deployed, QA submitted
4. Issue status is updated based on **actual outcomes**, not intent

**Fake dispatch (NEVER do this):**
- Adding "Dispatched for Implementation" comments to issues
- Moving issues to "In Progress" without spawning any agent
- Reporting issues as "dispatched but not started"

**If you cannot spawn implementation right now** (context pressure, session ending, too many items), be honest:
- Label it "ready-to-implement" (not "In Progress")
- Log: "Enriched and ready for next session to implement"
- Do NOT call it "dispatched"

**Why this matters:** Calling bookkeeping "dispatch" creates false status. The owner thinks work is happening. It isn't. The next session sees "In Progress" and assumes someone else is working on it. The issue rots in fake-progress limbo â€” worse than being honestly backlogged.

---

## TDD Requirements â€” Red-Green-Refactor

**The purpose of TDD is to PROVE the fix works before any human touches it.** Running `npm test` with passing unrelated tests is NOT TDD â€” that just proves you didn't break other things.

### The Three Phases (MANDATORY)

| Phase | What | Anti-pattern |
|-------|------|--------------|
| **RED** | Write/identify a test that reproduces the bug or validates the feature. Watch it FAIL. | Skipping straight to fixing code |
| **GREEN** | Implement the fix. Watch the test PASS. | "Tests pass" when no new test was written for the fix |
| **REFACTOR** | Clean up if needed. All tests still pass. | Shipping without running the full suite |

### What "Verify the Fix" Means

**A test or verification that proves the fix works is NOT optional.** Choose the appropriate level:

| Situation | Verification Type | Example |
|-----------|-------------------|---------|
| Logic bug (parsing, validation, calculation) | Unit test | `expect(parseResponse(malformed)).toEqual(fallback)` |
| API behavior (timeout, error handling, response) | Integration test | `expect(await callEndpoint(largePayload)).resolves.within(50000)` |
| Infrastructure/env-specific (can't automate) | Manual verification + evidence | `curl` the endpoint with realistic data, measure response time, include result |
| UI behavior | E2E test (Playwright) | Per project's e2e-testing skill |
| Performance/timeout | Math check + measurement | Calculate: input_size x processing_speed vs timeout_limit |

### The Verification Gate (Before QA Submission)

**Before submitting to QA, you MUST have evidence that the fix works:**

1. **Automated test written and passing** â€” preferred
2. **Manual verification performed and documented** â€” acceptable when automation isn't feasible (include the evidence: curl output, response time, screenshot)
3. **Mathematical proof** â€” for performance/timeout issues, show the math works

**INSUFFICIENT evidence (will result in wasted QA cycles):**
- "npm test passes" alone â€” proves no regression, NOT that the fix works
- "Config changed, should work now" â€” proves nothing without functional verification
- "Deployed, please test" â€” human QA is for UX/acceptance, not basic functionality

**If you cannot verify the fix works yourself, DO NOT submit to QA.** Investigate further. The human QA tester checks UX and acceptance â€” they are NOT your test runner for "does it work at all?"

### Anti-Pattern: Shipping Unverified Fixes to Human QA

**Real example (Issue #184 â€” Longs generation timeout):**

The autonomous bot changed timeout configuration **4 times**, ran `npm test` each time (all green), deployed and submitted to human QA each time. The human reported failure each time. 4 wasted QA cycles because the bot never once verified the endpoint actually completes within the timeout.

| Attempt | "Fix" | Verification | Result |
|---------|-------|-------------|--------|
| 1 | `.catch()` on JSON parse | `npm test` âœ… (unrelated) | QA fail: "Failed to fetch" |
| 2 | `maxDuration=60` | `npm test` âœ… (unrelated) | QA fail: "Generation failed:" |
| 3 | Move config to `vercel.json` | `npm test` âœ… (unrelated) | QA fail: 504 timeout |
| 4 | Timeout 60sâ†’50s | `npm test` âœ… (unrelated) | QA fail: 504 timeout |

**What should have happened:**
1. **Measure** actual endpoint response time with realistic data (large transcript)
2. **Compare** against timeout limit: if `response_time > timeout`, config changes won't help
3. **Identify root cause**: data too large for time budget â†’ reduce input size, not adjust timeout
4. **Fix root cause**, verify endpoint responds successfully, THEN submit to QA

**The rule:** If your fix is "change a number in config," you MUST verify the system actually works with that number before shipping.

### Standard TDD Checklist

| Step | Required |
|------|----------|
| Run existing test suite | Always (catch regressions) |
| Write/update test for THIS specific fix | When fix touches testable logic |
| Verify the fix works (test, curl, math) | **ALWAYS â€” no exceptions** |
| All tests pass | Must pass before deploy |
| Evidence in QA submission | Always (what you tested, what you observed) |

For `approval-required` repos, the test results AND verification evidence are part of the package presented to the developer before they approve deployment.

---

## Deployment Policy Awareness

The loop reads `Deploy mode` from the repo's bug-intake config and adjusts behavior:

### `auto-prod` (e.g., <PROJECT_C>)
- No stage environment. Tests pass locally â†’ deploy to production.
- Fast iteration. Acceptable risk because team is the only user.

### `auto-stage` (e.g., <PROJECT_B>)
- Stage environment exists. Default branch = stage.
- Push automatically after tests pass. Breaking stage is fine â€” fast iteration.

### `approval-required` (e.g., <COMPANY_A>)
- Production with live clients. No stage environment (yet).
- **The loop still starts automatically** â€” bugs are scanned, triaged, fixed, and tested without asking.
- **Deployment requires explicit developer approval.** Present:
  1. What was fixed
  2. Test results (pass/fail)
  3. Files changed
  4. Ask: "Deploy to production?"
- Developer says yes â†’ deploy. Developer says no â†’ log and move on.

---

## Actionable Slack Replies Only

Every Slack reply MUST be actionable. Never leave things hanging.

**If fix is deployed and needs testing:** Tell the reporter exactly what to test.
  - "Fix deployed. Please test: [specific steps]. Reply here if still broken."

**If deferred:** Be clear about what happens next.
  - "Noted for owner review. No action needed from you."

**Never:** Post vague "tracking this" messages that leave people wondering if they need to do anything.

---

## Wait Implementation

```bash
# 5-minute wait between iterations
sleep 300
```

Or on Windows:
```powershell
Start-Sleep -Seconds 300
```

---

## Response Deadline (Verification)

If a reporter hasn't confirmed or rejected a fix within **2 days**, @mention them in the thread asking for verification. This is checked on every scan.

Local overrides may add additional response deadline behavior (e.g., escalating to a manager).

---

## Local Override Pattern

Repos can provide `.claude/skills/dev-loop/SKILL.md` to extend this skill. Common overrides:

- **Additional scan targets** (e.g., separate QA channel)
- **Dedicated QA person** (e.g., always mention a specific team member)
- **Custom escalation** (e.g., escalate to CEO after N days)
- **Modified exit conditions** (e.g., different idle threshold)
- **Custom test steps** (e.g., E2E tests for UI changes)

The override pattern works the same as bug-intake: `## Override:` sections augment global steps, never skip them.

---

## Related Skills

| Skill | Location | Relationship |
|-------|----------|-------------|
| `autonomous-issue-dispatch` | `~/.claude/skills/` | Parent architecture â€” defines the full pipeline |
| `bug-intake` | `~/.claude/skills/` | **Core** â€” Each scan iteration runs bug-intake |
| `issue-dispatcher` | `~/.claude/skills/` | **Downstream** â€” Dispatcher triages issues |
| `slack` | `~/.claude/skills/` | Underlying API patterns for Slack operations |
| `qa-submission` | `.claude/skills/` (per-project) | Used by overrides that have separate QA channels |
